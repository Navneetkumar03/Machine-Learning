{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6b9e4117",
      "metadata": {
        "id": "6b9e4117"
      },
      "source": [
        "Assignment Code: DA-AG-012\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6146aa",
      "metadata": {
        "id": "ae6146aa"
      },
      "source": [
        "### Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "Ans. A Decision Tree is a supervised learning algorithm used for both classification and regression. In classification, it predicts the class label by learning simple decision rules inferred from the data features.\n",
        "\n",
        "### How it works:\n",
        "The tree starts at the root node, splits the data on feature values at internal nodes, and outputs a class label at the leaf node.\n",
        "\n",
        "At each node, the algorithm chooses the feature and threshold that best separate the data using impurity measures (like Gini or Entropy).\n",
        "\n",
        "### Example:\n",
        "Using the Iris dataset, a Decision Tree might first split on petal length â‰¤ 2.45 cm. If true, it's classified as Setosa; otherwise, the tree continues further splits to distinguish between Versicolor and Virginica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49dac4dd"
      },
      "source": [
        "##Question 2: Explain Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "Answer: Both are used to measure how mixed the class labels are in a node.\n",
        "\n",
        "### Gini Impurity:\n",
        "$Gini = 1 - \\sum p_i^2$, where $p_i$ is the probability of class $i$. Lower Gini means purer nodes.\n",
        "\n",
        "### Entropy:\n",
        "$Entropy = -\\sum p_i \\log_2(p_i)$. Measures the level of uncertainty.\n",
        "\n",
        "### Impact on splits:\n",
        "The algorithm selects the feature split that results in the largest reduction in impurity (Gini or Entropy). This reduction is called Information Gain.\n",
        "\n",
        "### Example:\n",
        "If node A has 50% class A and 50% class B, its Gini = 0.5 and Entropy = 1. A split that produces pure nodes (100% of a single class) would reduce impurity to 0."
      ],
      "id": "49dac4dd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "Answer:\n",
        "\n",
        "| Type        | Description                                                                  | Advantage                       |\n",
        "|-------------|------------------------------------------------------------------------------|---------------------------------|\n",
        "| Pre-Pruning | Stops the tree growth early by limiting depth, min samples, etc.             | Prevents overfitting early      |\n",
        "| Post-Pruning| Grows the full tree, then prunes back based on validation performance        | Results in simpler, better tree |\n",
        "\n",
        "Example:\n",
        "\n",
        "Pre-Pruning: `max_depth=3`, `min_samples_split=5`\n",
        "\n",
        "Post-Pruning: cost complexity pruning (`ccp_alpha`) after building a full tree"
      ],
      "metadata": {
        "id": "VMdCLOlDErHB"
      },
      "id": "VMdCLOlDErHB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "Answer:\n",
        "Information Gain is the decrease in impurity after a dataset is split on an attribute.\n",
        "\n",
        "Formula:\n",
        "$IG = Impurity_{parent} - \\sum \\left( \\frac{n_{child}}{n_{parent}} \\times Impurity_{child} \\right)$\n",
        "\n",
        "Why it's important:\n",
        "The feature that maximizes Information Gain is chosen to split the node, ensuring the best separation of classes.\n",
        "\n",
        "Example:\n",
        "If a split reduces Gini from 0.5 to 0.2, the Information Gain is 0.3, indicating an effective split."
      ],
      "metadata": {
        "id": "SFWtYdJJFiy5"
      },
      "id": "SFWtYdJJFiy5"
    },
    {
      "cell_type": "markdown",
      "id": "cc805659",
      "metadata": {
        "id": "cc805659"
      },
      "source": [
        "##Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Answer:\n",
        "### Applications:\n",
        "\n",
        "* Healthcare: disease prediction\n",
        "* Finance: loan approval, credit risk analysis\n",
        "* Marketing: customer segmentation\n",
        "* Operations: supply chain decisions\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "* Easy to interpret and visualize\n",
        "* No need for feature scaling\n",
        "* Handles both numerical and categorical data\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "* Prone to overfitting\n",
        "* Unstable with small changes in data\n",
        "* Biased toward features with more levels (for classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d59fe8f2",
      "metadata": {
        "id": "d59fe8f2"
      },
      "source": [
        "### Question 6: Load Iris Dataset, Train Decision Tree (Gini), Print Accuracy & Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3203da57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3203da57",
        "outputId": "6a85f083-4d7c-4e3c-97ce-29fe28cfa21d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b3979e9",
      "metadata": {
        "id": "0b3979e9"
      },
      "source": [
        "### Question 7: Train Iris Decision Tree (max_depth=3) vs Full Tree Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12284d79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12284d79",
        "outputId": "67e2cf46-e0a2-430f-d397-f27e4dc864b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.0\n",
            "Accuracy (full tree): 1.0\n"
          ]
        }
      ],
      "source": [
        "model_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "model_limited.fit(X_train, y_train)\n",
        "acc_limited = accuracy_score(y_test, model_limited.predict(X_test))\n",
        "\n",
        "model_full = DecisionTreeClassifier(random_state=42)\n",
        "model_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, model_full.predict(X_test))\n",
        "\n",
        "print(\"Accuracy (max_depth=3):\", acc_limited)\n",
        "print(\"Accuracy (full tree):\", acc_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76949e75",
      "metadata": {
        "id": "76949e75"
      },
      "source": [
        "### Question 8: Train Decision Tree Regressor on Boston Housing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3332344e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3332344e",
        "outputId": "7af70925-c2c1-4cae-e562-34fdeebd8d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Note: load_boston is deprecated, using California housing dataset\n",
        "boston = fetch_california_housing()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6566d5ae",
      "metadata": {
        "id": "6566d5ae"
      },
      "source": [
        "### Question 9: GridSearchCV for Decision Tree on Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "41a7e0ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41a7e0ad",
        "outputId": "2ba00ea8-933a-47b6-b6ca-310470476211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Best Accuracy on Validation: 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset again to ensure X_train and y_train are from Iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy on Validation:\", grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3afec8",
      "metadata": {
        "id": "af3afec8"
      },
      "source": [
        "### Question 10: End-to-End Process for Healthcare Prediction Using Decision Trees\n",
        "**Step-by-Step:**\n",
        "1. **Handle Missing Values**: Use `SimpleImputer` for numerical and categorical columns.\n",
        "2. **Encode Categorical Features**: Use `OneHotEncoder` or `OrdinalEncoder`.\n",
        "3. **Train Model**: Use `DecisionTreeClassifier` with tuned parameters.\n",
        "4. **Hyperparameter Tuning**: Use `GridSearchCV` to optimize `max_depth`, `min_samples_split`.\n",
        "5. **Evaluation**: Use metrics like Accuracy, Precision, Recall, and ROC-AUC.\n",
        "\n",
        "**Business Value:**\n",
        "- Early detection of disease\n",
        "- Better patient management\n",
        "- Cost-effective treatment allocation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}